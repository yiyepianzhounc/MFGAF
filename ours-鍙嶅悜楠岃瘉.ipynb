{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "164b4288-b10c-46a7-bfac-6c551fefae3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------- 开始初始化 bart 模型 ------------------------------------------\n",
      "----------------------------------- 结束初始化 bart 模型 ------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from bart_score import BARTScorer\n",
    "from sklearn.metrics import auc,roc_curve,roc_auc_score\n",
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 初始化 BARTScorer\n",
    "print(\"----------------------------------- 开始初始化 bart 模型 ------------------------------------------\")\n",
    "bartscorer = BARTScorer(device='cuda:0',checkpoint=\"facebook/bart-large-cnn\")\n",
    "print(\"----------------------------------- 结束初始化 bart 模型 ------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75333a8-a35e-4052-9a9b-e30b85c5822b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19afcfd2-766c-4834-ad4b-4934c2848d1e",
   "metadata": {},
   "source": [
    "# 1、读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2addec9a-d355-4e85-ba46-da34e4762dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 jsonl 文件\n",
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # 将每一行的 JSON 字符串转换为字典\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "\n",
    "# 读取 finance\n",
    "file_path = 'finance_samples_gpt4i-mini.jsonl'  \n",
    "finance_samples = read_jsonl(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5135a50-d4eb-4a45-90ec-e32c3e4ee8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce101d-a59d-4fb0-911b-c4426a362c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d2e7d05-45cb-4d7a-8cfb-05d1d578d9b0",
   "metadata": {},
   "source": [
    "# 2、数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66705dcc-cacf-4eca-a9b2-37ee07e08e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3933it [00:00, 55509.11it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_chinese(text):\n",
    "    # 使用正则表达式匹配所有中文字符并替换为空字符串\n",
    "    pattern = re.compile(r'[\\u4e00-\\u9fff]')\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "# 不同的数据合并\n",
    "question_full = []\n",
    "human_answers_full = []\n",
    "chatgpt_answers_full = []\n",
    "human_answers_masked_full = []\n",
    "chatgpt_answers_masked_full = []\n",
    "human_answers_masked_fill_full = []\n",
    "chatgpt_answers_masked_fill_full = []\n",
    "human_answers_revised_full = []\n",
    "chatgpt_answers_revised_full = []\n",
    "\n",
    "\n",
    "\n",
    "for i, sample in tqdm(enumerate(finance_samples)):\n",
    "    question = sample['question']\n",
    "    human_answers = sample['human_answers']\n",
    "    chatgpt_answers = sample['chatgpt_answers']\n",
    "    human_answers_masked = sample['human_answers_masked']\n",
    "    chatgpt_answers_masked = sample['chatgpt_answers_masked']\n",
    "    human_answers_masked_fill = sample['human_answers_masked_fill']\n",
    "    chatgpt_answers_masked_fill = sample['chatgpt_answers_masked_fill']\n",
    "    human_answers_revised = sample['human_answers_revised']\n",
    "    chatgpt_answers_revised = sample['chatgpt_answers_revised']\n",
    "\n",
    "    # 去除中文\n",
    "    human_answers_masked = remove_chinese(human_answers_masked)\n",
    "    chatgpt_answers_masked = remove_chinese(chatgpt_answers_masked)\n",
    "    human_answers_masked_fill = remove_chinese(human_answers_masked_fill)\n",
    "    chatgpt_answers_masked_fill = remove_chinese(chatgpt_answers_masked_fill)\n",
    "\n",
    "    # 存储\n",
    "    question_full.append(question)\n",
    "    human_answers_full.append(human_answers)\n",
    "    chatgpt_answers_full.append(chatgpt_answers)\n",
    "    human_answers_masked_full.append(human_answers_masked)\n",
    "    chatgpt_answers_masked_full.append(chatgpt_answers_masked)\n",
    "    human_answers_masked_fill_full.append(human_answers_masked_fill)\n",
    "    chatgpt_answers_masked_fill_full.append(chatgpt_answers_masked_fill)\n",
    "    human_answers_revised_full.append(human_answers_revised)\n",
    "    chatgpt_answers_revised_full.append(chatgpt_answers_revised)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce5616-5e3e-45fa-a186-af15b3ee93b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a16003e-ccbf-41b5-bf8c-3063fdec3f02",
   "metadata": {},
   "source": [
    "# 3、计算各种相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e409ff2-c192-4ac6-af03-f4f1588433c5",
   "metadata": {},
   "source": [
    "# 3.1 计算修改前后的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc7eabf3-da35-43be-b9ae-42ec84e45703",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------- 开始计算相似度 ------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------------------------- 开始计算相似度 ------------------------------------------\")\n",
    "chatgpt_revise_score = bartscorer.score(chatgpt_answers_revised_full, chatgpt_answers_full)\n",
    "human_revise_score = bartscorer.score(human_answers_revised_full, human_answers_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c79270c8-6c3b-47e9-b5d1-a2e016bfeff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the auroc is: 0.9739491685322336\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_score = []\n",
    "for i in range(0,len(chatgpt_revise_score)):\n",
    "    y_true.append(1)\n",
    "    y_score.append(chatgpt_revise_score[i])\n",
    "    \n",
    "for i in range(0,len(human_revise_score)):\n",
    "    y_true.append(0)\n",
    "    y_score.append(human_revise_score[i])\n",
    "\n",
    "# 计算评估指标\n",
    "auroc_score = roc_auc_score(y_true, y_score)\n",
    "print(\"the auroc is:\",auroc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67562ab-07c1-4a3c-b637-fbc35470dd3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb66e2b0-f61f-4f20-948a-f42052e7f19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------- 开始计算相似度 ------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 反过来\n",
    "print(\"----------------------------------- 开始计算相似度 ------------------------------------------\")\n",
    "chatgpt_revise_score = bartscorer.score(chatgpt_answers_full, chatgpt_answers_revised_full)\n",
    "human_revise_score = bartscorer.score(human_answers_full, human_answers_revised_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "231154c3-d37a-46c1-b183-2bd89e1b3dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the auroc is: 0.7912387240925731\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_score = []\n",
    "for i in range(0,len(chatgpt_revise_score)):\n",
    "    y_true.append(1)\n",
    "    y_score.append(chatgpt_revise_score[i])\n",
    "    \n",
    "for i in range(0,len(human_revise_score)):\n",
    "    y_true.append(0)\n",
    "    y_score.append(human_revise_score[i])\n",
    "\n",
    "# 计算评估指标\n",
    "auroc_score = roc_auc_score(y_true, y_score)\n",
    "print(\"the auroc is:\",auroc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70013f12-1a3e-4202-8a61-bda89b825dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d3de8-89b3-47bf-b8c1-1aeff2e30a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1c846a3-b77c-4b67-a6c6-80e432acb437",
   "metadata": {},
   "source": [
    "## 3.2 计算 mask 前后的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65a44ad0-e856-4da8-8a75-a52851b54e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------- 开始计算相似度 ------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------------------------- 开始计算相似度 ------------------------------------------\")\n",
    "chatgpt_mask_score = bartscorer.score(chatgpt_answers_masked_fill_full, chatgpt_answers_full)\n",
    "human_mask_score = bartscorer.score(human_answers_masked_fill_full, human_answers_full)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9105c6d-34aa-46f6-971f-8700a124863b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17e3808-e4c3-46cd-8b65-e6aa9d7069ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5345d10-d506-4758-ab83-85aeb542341c",
   "metadata": {},
   "source": [
    "# 4、各种相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e73fcd-90fd-4f30-8d88-73ef0eb972bb",
   "metadata": {},
   "source": [
    "## 4.1 语义相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee051b90-8371-408d-94cb-78d325138743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.8590004444122314\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 加载模型 \"\"\"\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# from modelscope import snapshot_download\n",
    "# model_dir = snapshot_download(\"AI-ModelScope/bge-large-en-v1.5\", cache_dir='pretrain_models', revision='master') # 加载模型\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"/root/autodl-tmp/ai_generated/LLM-generated-text-detection/pretrain_models/AI-ModelScope/bge-large-en-v1.5\")\n",
    "\n",
    "\n",
    "def embedding_similarity(text1, text2):\n",
    "    # 将文本转换为嵌入向量\n",
    "    text1_embedding = model.encode(text1, normalize_embeddings=True, device='cuda:0')\n",
    "    text2_embedding = model.encode(text2, normalize_embeddings=True, device='cuda:0')\n",
    "    \n",
    "    # 将嵌入向量转换为PyTorch张量\n",
    "    text1_embedding = torch.tensor(text1_embedding).to('cuda:0')\n",
    "    text2_embedding = torch.tensor(text2_embedding).to('cuda:0')\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    cosine_similarity = torch.nn.functional.cosine_similarity(text1_embedding, text2_embedding, dim=0)\n",
    "    \n",
    "    return cosine_similarity.item()\n",
    "\n",
    "# 示例使用\n",
    "text1 = \"This is a sample sentence.\"\n",
    "text2 = \"This is another example sentence.\"\n",
    "similarity = embedding_similarity(text1, text2)\n",
    "print(f\"Cosine Similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fd5311-8597-44c9-b43a-25892033f90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30b2ee9b-9f09-462a-aa9d-19d12033758d",
   "metadata": {},
   "source": [
    "## 4.2 语法一致性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ca4fb9a-e578-4df7-9bff-bef05c6b388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def syntactic_resonance_analysis(text_raw, text_transformed):\n",
    "    \"\"\"\n",
    "    Analyzes the syntactic similarity between two English sentences.\n",
    "\n",
    "    Args:\n",
    "        text_raw: The original text (str).\n",
    "        text_transformed: The transformed text (str).\n",
    "\n",
    "    Returns:\n",
    "        score_syn: The syntactic similarity score (float).\n",
    "    \"\"\"\n",
    "\n",
    "    doc_raw = nlp(text_raw)\n",
    "    doc_transformed = nlp(text_transformed)\n",
    "\n",
    "    # Extract syntactic features\n",
    "    # Feature 1: Dependency relation triples\n",
    "    dep_raw = [(token.head.i, token.i, token.dep_) for token in doc_raw]\n",
    "    dep_transformed = [(token.head.i, token.i, token.dep_) for token in doc_transformed]\n",
    "\n",
    "    # Feature 2: Part-of-speech (POS) tag sequence\n",
    "    pos_raw = [token.tag_ for token in doc_raw]  # Using detailed POS tags\n",
    "    pos_transformed = [token.tag_ for token in doc_transformed]\n",
    "\n",
    "    # Feature 3: Syntactic structure depth (a simple estimation)\n",
    "    depth_raw = max([token.i - token.head.i if token.head.i < token.i else token.head.i - token.i for token in doc_raw], default=0)\n",
    "    depth_transformed = max([token.i - token.head.i if token.head.i < token.i else token.head.i - token.i for token in doc_transformed], default=0)\n",
    "\n",
    "    # Calculate syntactic similarity score\n",
    "    # Similarity Metric 1: Proportion of common dependency relation triples\n",
    "    common_dep = len(set(dep_raw) & set(dep_transformed))\n",
    "    total_dep = max(len(dep_raw), len(dep_transformed))\n",
    "    dep_similarity = common_dep / total_dep if total_dep > 0 else 1.0\n",
    "\n",
    "    # Similarity Metric 2: Similarity of POS tag sequences (using proportion of matching tags)\n",
    "    common_pos = 0\n",
    "    min_len = min(len(pos_raw), len(pos_transformed))\n",
    "    for i in range(min_len):\n",
    "        if pos_raw[i] == pos_transformed[i]:\n",
    "            common_pos += 1\n",
    "    pos_similarity = common_pos / max(len(pos_raw), len(pos_transformed)) if max(len(pos_raw), len(pos_transformed)) > 0 else 1.0\n",
    "\n",
    "    # Similarity Metric 3: Difference in syntactic structure depth\n",
    "    depth_diff = abs(depth_raw - depth_transformed)\n",
    "    normalized_depth_diff = depth_diff / max(len(doc_raw), len(doc_transformed)) if max(len(doc_raw), len(doc_transformed)) > 0 else 0.0\n",
    "    depth_similarity = 1.0 - normalized_depth_diff\n",
    "\n",
    "    # Combine syntactic similarity scores (adjust weights as needed)\n",
    "    score_syn = (0.5 * dep_similarity + 0.3 * pos_similarity + 0.2 * depth_similarity)\n",
    "\n",
    "    return score_syn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c94788-9b9f-4f82-a34e-4806d5166100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d592590-1c6e-4dde-9fd7-2e0bdb1f1aa8",
   "metadata": {},
   "source": [
    "## 4.3 词汇一致性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fcb4f35-12fa-48ab-882d-43a45ce3fd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5384615384615384\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lexical_resonance_analysis(text_raw, text_transformed, top_n=10):\n",
    "    \"\"\"\n",
    "    Analyzes the lexical similarity between two English texts using TF-IDF and Jaccard index.\n",
    "\n",
    "    Args:\n",
    "        text_raw: The original text (str).\n",
    "        text_transformed: The transformed text (str).\n",
    "        top_n: The number of top keywords to consider (int).\n",
    "\n",
    "    Returns:\n",
    "        score_lex: The lexical similarity score (float).\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize TF-IDF vectorizer with English stop words\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    # Fit and transform the texts\n",
    "    tfidf_matrix = vectorizer.fit_transform([text_raw, text_transformed])\n",
    "\n",
    "    # Get the vocabulary (words)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Get TF-IDF scores for each text\n",
    "    tfidf_scores_raw = tfidf_matrix[0].toarray().flatten()\n",
    "    tfidf_scores_transformed = tfidf_matrix[1].toarray().flatten()\n",
    "\n",
    "    # Get top N keywords for each text\n",
    "    top_indices_raw = tfidf_scores_raw.argsort()[-top_n:][::-1]\n",
    "    top_keywords_raw = [feature_names[i] for i in top_indices_raw]\n",
    "\n",
    "    top_indices_transformed = tfidf_scores_transformed.argsort()[-top_n:][::-1]\n",
    "    top_keywords_transformed = [feature_names[i] for i in top_indices_transformed]\n",
    "\n",
    "    # Convert to sets\n",
    "    keywords_raw_set = set(top_keywords_raw)\n",
    "    keywords_transformed_set = set(top_keywords_transformed)\n",
    "\n",
    "    # Calculate Jaccard index\n",
    "    intersection = len(keywords_raw_set.intersection(keywords_transformed_set))\n",
    "    union = len(keywords_raw_set.union(keywords_transformed_set))\n",
    "\n",
    "    if union == 0:\n",
    "        score_lex = 0.0\n",
    "    else:\n",
    "        score_lex = intersection / union\n",
    "\n",
    "    return score_lex\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text_raw = \"Using a pre-trained sentence embedding model, such as Sentence-BERT, generate dense vector representations for the original text.\"\n",
    "    text_transformed = \"Dense vector representations for the original text are generated using a pre-trained sentence embedding model like Sentence-BERT.\"\n",
    "\n",
    "    score = lexical_resonance_analysis(text_raw, text_transformed)\n",
    "    print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e287f-a662-42bd-ac66-7d97fea938c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbe040df-f2c3-40ed-946e-d43e338b53e7",
   "metadata": {},
   "source": [
    "## 4.4 推理一致性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366db5a3-1e8e-40a6-a6d0-8e668bfd63d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "369a2bd8-f88b-461f-92e4-ce5910002063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推理共振分析的一致性分数: 0.7328\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# 加载预训练的NLI模型\n",
    "nli_model = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "def reasoning_analysis(text1, text2):\n",
    "    \n",
    "    # 定义可能的标签\n",
    "    labels = [\"entailment\", \"contradiction\", \"neutral\"]\n",
    "    \n",
    "    # 进行NLI分类\n",
    "    result = nli_model(text2, candidate_labels=labels, hypothesis_template=\"This text is {}.\")\n",
    "    \n",
    "    # 获取entailment的概率作为一致性分数\n",
    "    consistency_score = result['scores'][result['labels'].index('entailment')]\n",
    "    \n",
    "    return consistency_score\n",
    "\n",
    "def calculate_consistency(paragraph1, paragraph2):\n",
    "    # 计算双向的一致性分数\n",
    "    score1 = reasoning_analysis(paragraph1, paragraph2)\n",
    "    score2 = reasoning_analysis(paragraph2, paragraph1)\n",
    "    \n",
    "    # 取平均值作为最终的一致性分数\n",
    "    final_score = (score1 + score2) / 2\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "# 使用示例\n",
    "paragraph1 = \"Input your first paragraph here.\"\n",
    "paragraph2 = \"Input your second paragraph here.\"\n",
    "\n",
    "consistency_score = calculate_consistency(paragraph1, paragraph2)\n",
    "print(f\"推理共振分析的一致性分数: {consistency_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d595dd4a-2d00-4119-b8a0-e664d735fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computer_scores(text1, text2):\n",
    "\n",
    "    \"\"\" 计算分数 \"\"\"\n",
    "    # 语义一致性\n",
    "    # semantics_score = embedding_similarity(text2, text1)\n",
    "\n",
    "    # bart 的语义一致性\n",
    "    semantics_score = bartscorer.score([text2], [text1])[0]\n",
    "    \n",
    "\n",
    "    # 语法一致性\n",
    "    syntactic_score = syntactic_resonance_analysis(text1, text2)\n",
    "\n",
    "    # 词汇一致性\n",
    "    lexical_score = lexical_resonance_analysis(text1, text2)\n",
    "\n",
    "    # 推理一致性\n",
    "    reasoning_score = reasoning_analysis(text1, text2)\n",
    "\n",
    "    score_dict = {\n",
    "        'semantics_score': semantics_score,\n",
    "        'syntactic_score': syntactic_score,\n",
    "        'lexical_score': lexical_score,\n",
    "        'reasoning_score': reasoning_score\n",
    "    }\n",
    "    \n",
    "    return score_dict\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39788845-7403-4976-9dcc-1890020e5723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a42a85-44f7-4cc8-baf9-72a387535ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa438337-ba65-4106-8a02-fa1c06f073f6",
   "metadata": {},
   "source": [
    "# 5、计算 重写 的分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9aadab92-a0d5-44f7-935c-f58a173f887d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3933/3933 [18:48<00:00,  3.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# 存储最终预测的 logits\n",
    "chatgpt_scores_dict_revised_full = []\n",
    "human_scores_dict_revised_full = []\n",
    "\n",
    "chatgpt_scores_dict_masked_fill_full = []\n",
    "human_scores_dict_masked_fill_full = []\n",
    "\n",
    "for i in tqdm(range(0,len(chatgpt_answers_revised_full))):\n",
    "\n",
    "    # 原始的\n",
    "    chatgpt_answers = chatgpt_answers_full[i]\n",
    "    human_answers = human_answers_full[i]\n",
    "\n",
    "    # 重写后的\n",
    "    chatgpt_answers_revised = chatgpt_answers_revised_full[i]\n",
    "    human_answers_revised = human_answers_revised_full[i]\n",
    "\n",
    "    # mask 填充后的\n",
    "    chatgpt_answers_masked_fill = chatgpt_answers_masked_fill_full[i]\n",
    "    human_answers_masked_fill = human_answers_masked_fill_full[i]\n",
    "\n",
    "    \"\"\" 计算重写的分数 \"\"\"\n",
    "    chatgpt_scores_dict_revised = computer_scores(chatgpt_answers, chatgpt_answers_revised)\n",
    "    human_scores_dict_revised = computer_scores(human_answers, human_answers_revised)\n",
    "\n",
    "    chatgpt_scores_dict_revised_full.append(chatgpt_scores_dict_revised)\n",
    "    human_scores_dict_revised_full.append(human_scores_dict_revised)\n",
    "\n",
    "    \n",
    "    \"\"\" 计算mask的分数 \"\"\"\n",
    "    chatgpt_scores_dict_masked_fill = computer_scores(chatgpt_answers, chatgpt_answers_masked_fill)\n",
    "    human_scores_dict_masked_fill = computer_scores(human_answers, human_answers_masked_fill)\n",
    "\n",
    "    chatgpt_scores_dict_masked_fill_full.append(chatgpt_scores_dict_masked_fill)\n",
    "    human_scores_dict_masked_fill_full.append(human_scores_dict_masked_fill)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa864525-2a17-4702-a401-48f6e8b14aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b30530-11cd-4326-b43a-4715725be5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d962cbd-8a22-4596-9cc2-1c7c7b20527d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e2300-2e76-44fd-97bc-872a8bcdcbde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d74cb9b-7406-423c-888c-783bcf44760f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41584117-a3e3-4b4f-90cf-869cc02aad94",
   "metadata": {},
   "source": [
    "## 5.2 聚合多个不同的分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bde7ad4e-b9fc-4d61-b39d-38b104fc6701",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Maybe you meant '==' or ':=' instead of '='? (1030795496.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[75], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    semantics_score_weight=0.52, syntactic_score_weight=0.0, lexical_score_weight=0.0, reasoning_score_weight=0.48\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "chatgpt_total_scores = []\n",
    "human_total_scores = []\n",
    "\n",
    "semantics_score_weight=0.52\n",
    "syntactic_score_weight=0.0\n",
    "lexical_score_weight=0.0\n",
    "reasoning_score_weight=0.48\n",
    "\n",
    "for i, _ in tqdm(enumerate(chatgpt_scores_dict_revised_full)):\n",
    "\n",
    "    # revised\n",
    "    chatgpt_revised_scores_dict = chatgpt_scores_dict_revised_full[i]\n",
    "    human_revised_scores_dict = human_scores_dict_revised_full[i]\n",
    "\n",
    "    chatgpt_revised_semantics_score = chatgpt_revised_scores_dict['semantics_score']\n",
    "    chatgpt_revised_syntactic_score = chatgpt_revised_scores_dict['syntactic_score']\n",
    "    chatgpt_revised_lexical_score = chatgpt_revised_scores_dict['lexical_score']\n",
    "    chatgpt_revised_reasoning_score = chatgpt_revised_scores_dict['reasoning_score']\n",
    "\n",
    "    human_revised_semantics_score = human_revised_scores_dict['semantics_score']\n",
    "    human_revised_syntactic_score = human_revised_scores_dict['syntactic_score']\n",
    "    human_revised_lexical_score = human_revised_scores_dict['lexical_score']\n",
    "    human_revised_reasoning_score = human_revised_scores_dict['reasoning_score']\n",
    "\n",
    "    \n",
    "    # masked_fill\n",
    "    chatgpt_masked_fill_scores_dict = chatgpt_scores_dict_masked_fill_full[i]\n",
    "    human_masked_fill_scores_dict = human_scores_dict_masked_fill_full[i]\n",
    "\n",
    "    chatgpt_masked_fill_semantics_score = chatgpt_masked_fill_scores_dict['semantics_score']\n",
    "    chatgpt_masked_fill_syntactic_score = chatgpt_masked_fill_scores_dict['syntactic_score']\n",
    "    chatgpt_masked_fill_lexical_score = chatgpt_masked_fill_scores_dict['lexical_score']\n",
    "    chatgpt_masked_fill_reasoning_score = chatgpt_masked_fill_scores_dict['reasoning_score']\n",
    "\n",
    "    human_masked_fill_semantics_score = human_masked_fill_scores_dict['semantics_score']\n",
    "    human_masked_fill_syntactic_score = human_masked_fill_scores_dict['syntactic_score']\n",
    "    human_masked_fill_lexical_score = human_masked_fill_scores_dict['lexical_score']\n",
    "    human_masked_fill_reasoning_score = human_masked_fill_scores_dict['reasoning_score']\n",
    "\n",
    "    \n",
    "    # 分数聚合\n",
    "    revised_chatgpt_total_score = semantics_score_weight * chatgpt_revised_semantics_score + \\\n",
    "                    syntactic_score_weight * chatgpt_revised_syntactic_score + \\\n",
    "                    lexical_score_weight * chatgpt_revised_lexical_score + \\\n",
    "                    reasoning_score_weight * chatgpt_revised_reasoning_score \n",
    "\n",
    "    revised_human_total_score = semantics_score_weight * human_revised_semantics_score + \\\n",
    "                    syntactic_score_weight * human_revised_syntactic_score + \\\n",
    "                    lexical_score_weight * human_revised_lexical_score + \\\n",
    "                    reasoning_score_weight * human_revised_reasoning_score \n",
    "\n",
    "    \n",
    "\n",
    "    # 存储\n",
    "    chatgpt_total_scores.append(revised_chatgpt_total_score)\n",
    "    human_total_scores.append(revised_human_total_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8336b-b955-411f-a3f7-2b143edf5eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad4210-ca8e-4494-bd65-8c95d3f68da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8cd968d2-6571-41e0-9376-8189656fc689",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Maybe you meant '==' or ':=' instead of '='? (2012589049.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[73], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    semantics_score_weight=0.52, syntactic_score_weight=0.0, lexical_score_weight=0.0, reasoning_score_weight=0.48\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "def get_fusion_score(chatgpt_scores_dict_revised_full=chatgpt_scores_dict_revised_full,\n",
    "                    human_scores_dict_revised_full=human_scores_dict_revised_full,\n",
    "                    ):\n",
    "    \n",
    "    chatgpt_total_scores = []\n",
    "    human_total_scores = []\n",
    "\n",
    "    semantics_score_weight=0.52, syntactic_score_weight=0.0, lexical_score_weight=0.0, reasoning_score_weight=0.48\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i, _ in tqdm(enumerate(chatgpt_scores_dict_revised_full)):\n",
    "    \n",
    "        # revised\n",
    "        chatgpt_revised_scores_dict = chatgpt_scores_dict_revised_full[i]\n",
    "        human_revised_scores_dict = human_scores_dict_revised_full[i]\n",
    "    \n",
    "        chatgpt_revised_semantics_score = chatgpt_revised_scores_dict['semantics_score']\n",
    "        chatgpt_revised_syntactic_score = chatgpt_revised_scores_dict['syntactic_score']\n",
    "        chatgpt_revised_lexical_score = chatgpt_revised_scores_dict['lexical_score']\n",
    "        chatgpt_revised_reasoning_score = chatgpt_revised_scores_dict['reasoning_score']\n",
    "    \n",
    "        human_revised_semantics_score = human_revised_scores_dict['semantics_score']\n",
    "        human_revised_syntactic_score = human_revised_scores_dict['syntactic_score']\n",
    "        human_revised_lexical_score = human_revised_scores_dict['lexical_score']\n",
    "        human_revised_reasoning_score = human_revised_scores_dict['reasoning_score']\n",
    "    \n",
    "        \n",
    "        # masked_fill\n",
    "        chatgpt_masked_fill_scores_dict = chatgpt_scores_dict_masked_fill_full[i]\n",
    "        human_masked_fill_scores_dict = human_scores_dict_masked_fill_full[i]\n",
    "    \n",
    "        chatgpt_masked_fill_semantics_score = chatgpt_masked_fill_scores_dict['semantics_score']\n",
    "        chatgpt_masked_fill_syntactic_score = chatgpt_masked_fill_scores_dict['syntactic_score']\n",
    "        chatgpt_masked_fill_lexical_score = chatgpt_masked_fill_scores_dict['lexical_score']\n",
    "        chatgpt_masked_fill_reasoning_score = chatgpt_masked_fill_scores_dict['reasoning_score']\n",
    "    \n",
    "        human_masked_fill_semantics_score = human_masked_fill_scores_dict['semantics_score']\n",
    "        human_masked_fill_syntactic_score = human_masked_fill_scores_dict['syntactic_score']\n",
    "        human_masked_fill_lexical_score = human_masked_fill_scores_dict['lexical_score']\n",
    "        human_masked_fill_reasoning_score = human_masked_fill_scores_dict['reasoning_score']\n",
    "        \n",
    "        # 分数聚合\n",
    "        revised_chatgpt_total_score = semantics_score_weight * chatgpt_revised_semantics_score + \\\n",
    "                        syntactic_score_weight * chatgpt_revised_syntactic_score + \\\n",
    "                        lexical_score_weight * chatgpt_revised_lexical_score + \\\n",
    "                        reasoning_score_weight * chatgpt_revised_reasoning_score \n",
    "\n",
    "        revised_human_total_score = semantics_score_weight * human_revised_semantics_score + \\\n",
    "                        syntactic_score_weight * human_revised_syntactic_score + \\\n",
    "                        lexical_score_weight * human_revised_lexical_score + \\\n",
    "                        reasoning_score_weight * human_revised_reasoning_score \n",
    "\n",
    "        \n",
    "    \n",
    "        # 存储\n",
    "        chatgpt_total_scores.append(revised_chatgpt_total_score)\n",
    "        human_total_scores.append(revised_human_total_score)\n",
    "\n",
    "\n",
    "        y_true = []\n",
    "        y_score = []\n",
    "        for i in range(0,len(chatgpt_total_scores)):\n",
    "            y_true.append(1)\n",
    "            y_score.append(chatgpt_total_scores[i])\n",
    "            \n",
    "        for i in range(0,len(human_total_scores)):\n",
    "            y_true.append(0)\n",
    "            y_score.append(human_total_scores[i])\n",
    "        \n",
    "        # 计算评估指标\n",
    "        auroc_score = roc_auc_score(y_true, y_score)\n",
    "        print(f\"semantics_score_weight={semantics_score_weight}, syntactic_score_weight={syntactic_score_weight}, lexical_score_weight={lexical_score_weight}, reasoning_score_weight={reasoning_score_weight}; \\n the auroc is:\",auroc_score)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a26df46-4cc5-4bd8-9099-1af3634023ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8824bbdb-87c8-4ead-9c3f-e937e052a66d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f561ffcf-174f-4a98-8d97-4471bc36a90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4c8ba865-6b40-4a5c-908c-4b3c6b32b4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the auroc is: 0.9739491038846781\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_score = []\n",
    "for i in range(0,len(chatgpt_total_scores)):\n",
    "    y_true.append(1)\n",
    "    y_score.append(chatgpt_total_scores[i])\n",
    "    \n",
    "for i in range(0,len(human_total_scores)):\n",
    "    y_true.append(0)\n",
    "    y_score.append(human_total_scores[i])\n",
    "\n",
    "# 计算评估指标\n",
    "auroc_score = roc_auc_score(y_true, y_score)\n",
    "print(\"the auroc is:\",auroc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05e728-2426-4564-bf59-53520763f80c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b663bc-6ed1-405f-9c90-a9c2812eb1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb4a0bff-cb0a-4e5e-a485-ac105053637e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------- 开始计算相似度 ------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------------------------- 开始计算相似度 ------------------------------------------\")\n",
    "chatgpt_revise_score = bartscorer.score(chatgpt_answers_revised_full, chatgpt_answers_full)\n",
    "human_revise_score = bartscorer.score(human_answers_revised_full, human_answers_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd697469-435c-41e2-b97f-cc735ec8c386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the auroc is: 0.9739491685322336\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_score = []\n",
    "for i in range(0,len(chatgpt_revise_score)):\n",
    "    y_true.append(1)\n",
    "    y_score.append(chatgpt_revise_score[i])\n",
    "    \n",
    "for i in range(0,len(human_revise_score)):\n",
    "    y_true.append(0)\n",
    "    y_score.append(human_revise_score[i])\n",
    "\n",
    "# 计算评估指标\n",
    "auroc_score = roc_auc_score(y_true, y_score)\n",
    "print(\"the auroc is:\",auroc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98131286-6b20-427a-87cb-fba566d56351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98494b15-e5b0-4bf2-b9fb-455f79ad51b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece40a2e-2965-4640-a31a-903037dc10b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92117c10-8fb4-4ce9-b870-e07ddaedfafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94dafef-3fce-44a8-8aaf-b2b80898e64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bdf616-05c5-48d7-9977-0b98ec70f209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad84ad6-bbeb-4290-8309-0bec1064ffbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d6ea17-c599-41a0-8ecd-83a393610950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230e2d9-9bd1-4bb6-89de-cab4c3dfac0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6229e3f4-d644-4da3-a1f9-4653b5823104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4940b657-ee53-41db-bf97-b608427b4b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d75ac2-bc14-4d54-a420-85d0e3fb6484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980034f1-a207-4603-a45c-73cf0c63788c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13122190-f25a-42dd-b670-6270c9bc05b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e52ebd5-429d-4d89-a4bd-ff289194046a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2333f66-1da8-481f-acfa-96180147a73b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698838e1-0654-44ac-982d-e8b280ac64b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd7e34-ad58-4c6e-999f-1604ef4b8f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2a99e9a8-8955-43b1-8a69-7443e4322302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21999/21999 [03:05<00:00, 118.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best weights: semantics_score_weight=0.52, syntactic_score_weight=0.0, lexical_score_weight=0.0, reasoning_score_weight=0.48\n",
      "Best AUROC: 0.9767874548057021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def grid_search(chatgpt_scores_dict_revised_full, human_scores_dict_revised_full, \n",
    "                chatgpt_scores_dict_masked_fill_full, human_scores_dict_masked_fill_full):\n",
    "    # 定义权重的搜索范围\n",
    "    weight_range = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "    step = 0.02\n",
    "    weight_range = [round(i * step, 2) for i in range(0, int(1/step) + 1)]\n",
    "    \n",
    "    # 生成所有可能的权重组合\n",
    "    weight_combinations = list(product(weight_range, repeat=4))\n",
    "    \n",
    "    # 过滤掉权重和不等于1的组合\n",
    "    weight_combinations = [comb for comb in weight_combinations if sum(comb) == 1.0]\n",
    "    \n",
    "    best_auroc = 0\n",
    "    best_weights = None\n",
    "    \n",
    "    # 遍历所有权重组合\n",
    "    for weights in tqdm(weight_combinations):\n",
    "        semantics_score_weight, syntactic_score_weight, lexical_score_weight, reasoning_score_weight = weights\n",
    "        \n",
    "        # 调用get_fusion_score函数计算AUROC\n",
    "        auroc_score = get_fusion_score(\n",
    "            semantics_score_weight=semantics_score_weight,\n",
    "            syntactic_score_weight=syntactic_score_weight,\n",
    "            lexical_score_weight=lexical_score_weight,\n",
    "            reasoning_score_weight=reasoning_score_weight,\n",
    "            chatgpt_scores_dict_revised_full=chatgpt_scores_dict_revised_full,\n",
    "            human_scores_dict_revised_full=human_scores_dict_revised_full,\n",
    "            chatgpt_scores_dict_masked_fill_full=chatgpt_scores_dict_masked_fill_full,\n",
    "            human_scores_dict_masked_fill_full=human_scores_dict_masked_fill_full\n",
    "        )\n",
    "        \n",
    "        # 更新最佳AUROC和对应的权重\n",
    "        if auroc_score > best_auroc:\n",
    "            best_auroc = auroc_score\n",
    "            best_weights = weights\n",
    "    \n",
    "    print(f\"Best weights: semantics_score_weight={best_weights[0]}, syntactic_score_weight={best_weights[1]}, lexical_score_weight={best_weights[2]}, reasoning_score_weight={best_weights[3]}\")\n",
    "    print(f\"Best AUROC: {best_auroc}\")\n",
    "\n",
    "def get_fusion_score(semantics_score_weight, syntactic_score_weight, lexical_score_weight, reasoning_score_weight,\n",
    "                     chatgpt_scores_dict_revised_full, human_scores_dict_revised_full,\n",
    "                     chatgpt_scores_dict_masked_fill_full, human_scores_dict_masked_fill_full):\n",
    "    \n",
    "    chatgpt_total_scores = []\n",
    "    human_total_scores = []\n",
    "    \n",
    "    for i, _ in enumerate(chatgpt_scores_dict_revised_full):\n",
    "        # revised\n",
    "        chatgpt_revised_scores_dict = chatgpt_scores_dict_revised_full[i]\n",
    "        human_revised_scores_dict = human_scores_dict_revised_full[i]\n",
    "    \n",
    "        chatgpt_revised_semantics_score = chatgpt_revised_scores_dict['semantics_score']\n",
    "        chatgpt_revised_syntactic_score = chatgpt_revised_scores_dict['syntactic_score']\n",
    "        chatgpt_revised_lexical_score = chatgpt_revised_scores_dict['lexical_score']\n",
    "        chatgpt_revised_reasoning_score = chatgpt_revised_scores_dict['reasoning_score']\n",
    "    \n",
    "        human_revised_semantics_score = human_revised_scores_dict['semantics_score']\n",
    "        human_revised_syntactic_score = human_revised_scores_dict['syntactic_score']\n",
    "        human_revised_lexical_score = human_revised_scores_dict['lexical_score']\n",
    "        human_revised_reasoning_score = human_revised_scores_dict['reasoning_score']\n",
    "    \n",
    "        # masked_fill\n",
    "        chatgpt_masked_fill_scores_dict = chatgpt_scores_dict_masked_fill_full[i]\n",
    "        human_masked_fill_scores_dict = human_scores_dict_masked_fill_full[i]\n",
    "    \n",
    "        chatgpt_masked_fill_semantics_score = chatgpt_masked_fill_scores_dict['semantics_score']\n",
    "        chatgpt_masked_fill_syntactic_score = chatgpt_masked_fill_scores_dict['syntactic_score']\n",
    "        chatgpt_masked_fill_lexical_score = chatgpt_masked_fill_scores_dict['lexical_score']\n",
    "        chatgpt_masked_fill_reasoning_score = chatgpt_masked_fill_scores_dict['reasoning_score']\n",
    "    \n",
    "        human_masked_fill_semantics_score = human_masked_fill_scores_dict['semantics_score']\n",
    "        human_masked_fill_syntactic_score = human_masked_fill_scores_dict['syntactic_score']\n",
    "        human_masked_fill_lexical_score = human_masked_fill_scores_dict['lexical_score']\n",
    "        human_masked_fill_reasoning_score = human_masked_fill_scores_dict['reasoning_score']\n",
    "        \n",
    "        # 分数聚合\n",
    "        revised_chatgpt_total_score = semantics_score_weight * chatgpt_revised_semantics_score + \\\n",
    "                        syntactic_score_weight * chatgpt_revised_syntactic_score + \\\n",
    "                        lexical_score_weight * chatgpt_revised_lexical_score + \\\n",
    "                        reasoning_score_weight * chatgpt_revised_reasoning_score \n",
    "\n",
    "        revised_human_total_score = semantics_score_weight * human_revised_semantics_score + \\\n",
    "                        syntactic_score_weight * human_revised_syntactic_score + \\\n",
    "                        lexical_score_weight * human_revised_lexical_score + \\\n",
    "                        reasoning_score_weight * human_revised_reasoning_score \n",
    "\n",
    "        # 存储\n",
    "        chatgpt_total_scores.append(revised_chatgpt_total_score)\n",
    "        human_total_scores.append(revised_human_total_score)\n",
    "\n",
    "    y_true = []\n",
    "    y_score = []\n",
    "    for i in range(0, len(chatgpt_total_scores)):\n",
    "        y_true.append(1)\n",
    "        y_score.append(chatgpt_total_scores[i])\n",
    "            \n",
    "    for i in range(0, len(human_total_scores)):\n",
    "        y_true.append(0)\n",
    "        y_score.append(human_total_scores[i])\n",
    "        \n",
    "    # 计算评估指标\n",
    "    auroc_score = roc_auc_score(y_true, y_score)\n",
    "    return auroc_score\n",
    "\n",
    "# 示例调用\n",
    "grid_search(chatgpt_scores_dict_revised_full, human_scores_dict_revised_full, \n",
    "            chatgpt_scores_dict_masked_fill_full, human_scores_dict_masked_fill_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5439d0f3-b0ca-415c-b579-8245d782f786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
